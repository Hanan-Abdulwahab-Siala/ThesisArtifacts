{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb8fefc-5444-42b2-92f1-99c0fb0fd4a5",
   "metadata": {},
   "source": [
    "## This program inferring OCL by LLM4Models   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a21236-6d5b-4b5b-9c92-5608709125fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 3 to abstract OCL from Java\n",
    "# Choose 4 to abstract OCL from Python\n",
    "\n",
    "What_I_Want=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febe781-d01b-478c-86ff-84c78592601b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import time\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183cd0e-b089-4a8f-812b-0aa341a2c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d9607-d604-43f9-bd4d-5ce2ce70d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose version 1 or version 2 \n",
    "if What_I_Want==3:\n",
    "   peft_model_id='HA-Siala/Java-OCL-V1' # or\n",
    "   #peft_model_id='HA-Siala/Java-OCL-V2' \n",
    "else:\n",
    "   peft_model_id='HA-Siala/Python-OCL-V1' # or\n",
    "   #peft_model_id='HA-Siala/Python-OCL-V2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca715d0e-2f80-4a58-b9db-b3be398a28be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"mistralai/Mistral-7B-v0.3\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = \"left\"  \n",
    "tokenizer.pad_token = tokenizer.unk_token  \n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   peft_model_id,  \n",
    "   torch_dtype=torch.float16,\n",
    "   device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988eee15-f839-480c-8803-6bcee44f18c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePrompt(content):\n",
    "   if What_I_Want==3:\n",
    "      Instruction=\"\"\"Generate an Object Constraint Language (OCL) specification for the provided Java code. The output should:\n",
    "1. Ensure no repeated or redundant operations or classes.\n",
    "2. Include only the OCL code for the provided Java code.\n",
    "3. Do not include statements for items not found in the Java code.\"\"\"\n",
    "   else:\n",
    "      Instruction=\"\"\"Generate an Object Constraint Language (OCL) specification for the provided Python code. The output should:\n",
    "1. Ensure no repeated or redundant operations or classes.\n",
    "2. Include only the OCL code for the provided Python code.\n",
    "3. Do not include statements for items not found in the Python code.\"\"\"\n",
    "\n",
    "   Prompt=\"\"\"<s>[INST] Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately solves the following Task:\"\"\".strip()\n",
    "   return f\"\"\"{Prompt} \\n\n",
    "### Instruction:\n",
    "{Instruction}\n",
    "\n",
    "### Input:\n",
    "{content}\n",
    "[/INST]\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22923f6-d4b8-47fb-99dc-e1a90cdbfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateInferenceOutput(text): \n",
    "   tokenizer.pad_token = tokenizer.unk_token  \n",
    "   inputs = tokenizer(GeneratePrompt(text), return_tensors=\"pt\").to(DEVICE)\n",
    "   inputs_length = len(inputs[\"input_ids\"][0])\n",
    "   with torch.inference_mode():\n",
    "      outputs = model.generate(\n",
    "         **inputs,  \n",
    "         max_new_tokens= 8000, \n",
    "         temperature= 0.2, \n",
    "         do_sample= True, \n",
    "         pad_token_id=tokenizer.eos_token_id,\n",
    "         top_p=0.9,\n",
    "         use_cache=True,\n",
    "      )\n",
    "      output_p = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "      if output_p:\n",
    "         output_text = output_p[0]\n",
    "         split_text_p = output_text.split(\"Response:\")\n",
    "         return split_text_p[1].strip() if len(split_text_p) > 1 else None\n",
    "      else:\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ddb453-7909-4151-b4fc-044b33b6e967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the program here between \"\"\" and \"\"\"\n",
    "# You need to remove comments and empty lines (pre-processing)\n",
    "content=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578c20e-cf56-4c0d-bd4e-5439edf658bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "StartTime=time.time()\n",
    "Output = GenerateInferenceOutput(content)\n",
    "EndTime=time.time()\n",
    "InferenceTime=EndTime - StartTime\n",
    "print(Output)\n",
    "print()\n",
    "print(f\"Inference Time: {InferenceTime:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
