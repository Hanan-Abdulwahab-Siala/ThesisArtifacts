{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0632ef1b-10df-497d-a53a-e1a40453e4b3",
   "metadata": {},
   "source": [
    "## This program inferring UML by LLM4Models   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484462b-6069-49f2-80da-0fb995ad07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 1 to abstract UML from Java\n",
    "# Choose 2 to abstract UML from Python\n",
    "\n",
    "What_I_Want=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b1d5b-c8f9-442b-aaad-79bed1dab889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db12746-08b2-4d4b-a318-d3cf9f41853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d30416-347f-4770-976a-2ec30c3b895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_checkpoint = \"mistralai/Mistral-7B-v0.3\"\n",
    "model = AutoModelForCausalLM.from_pretrained(mistral_checkpoint, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, use_fast=True, add_prefix_space=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token   \n",
    "tokenizer.padding_side = \"left\" \n",
    "model.eval() \n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4da312-2662-4dee-9482-a539d60f9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if What_I_Want==1:\n",
    "   CheckPoint='HA-Siala/Java-UML' \n",
    "else:      \n",
    "   CheckPoint='HA-Siala/Python-UML' \n",
    "    \n",
    "peft_model = PeftModel.from_pretrained(model, CheckPoint, torch_dtype=torch.bfloat16, is_trainable=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634c7d5-a4ef-4f01-b332-aa4849e82f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeneratePrompt(content):\n",
    "   if What_I_Want==1:\n",
    "      Instruction=\"\"\"Generate a concise UML class diagram for the provided Java code. The output should:\n",
    "1. Define each class and interface only once, including its attributes, methods, and relationships.\n",
    "2. Include all relationships (Inheritance, Realization, Dependency, Association, Composition, Aggregation) without duplication.\n",
    "3. Avoid redundant or repeated operations, classes, or relationships.\"\"\"\n",
    "   else:\n",
    "      Instruction=\"\"\"Generate a concise UML class diagram for the provided Python code. The output should:\n",
    "1. Define each class and interface only once, including its attributes, methods, and relationships.\n",
    "2. Include all relationships (Inheritance, Realization, Dependency, Association, Composition, Aggregation) without duplication.\n",
    "3. Avoid redundant or repeated operations, classes, or relationships.\"\"\"\n",
    "\n",
    "   prompt = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response, which is in JSON format that appropriately solves the following Task:\n",
    "\n",
    "### Instruction:\n",
    "{Instruction}\n",
    "\n",
    "### Input:\n",
    "{content}\n",
    "\n",
    "### Response:\n",
    "\"\"\"   \n",
    "   return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398e3df-4f46-4931-9bba-23cc0e01b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateInferenceOutput(text): \n",
    "   tokenizer.pad_token = tokenizer.unk_token  \n",
    "   inputs = tokenizer(GeneratePrompt(text), return_tensors=\"pt\").to(DEVICE)\n",
    "   inputs_length = len(inputs[\"input_ids\"][0])\n",
    "   with torch.inference_mode():\n",
    "      outputs = model.generate(\n",
    "         **inputs,  \n",
    "         max_new_tokens= 8000, \n",
    "         temperature= 0.2, \n",
    "         do_sample= True, \n",
    "         pad_token_id=tokenizer.eos_token_id,\n",
    "         top_p=0.9,\n",
    "         use_cache=True,\n",
    "      )\n",
    "      output_p = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "      if output_p:\n",
    "         output_text = output_p[0]\n",
    "         split_text_p = output_text.split(\"Response:\")\n",
    "         return split_text_p[1].strip() if len(split_text_p) > 1 else None\n",
    "      else:\n",
    "         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbf0a6-a1b8-44ac-9d42-ff8b37cc5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the program here between \"\"\" and \"\"\"\n",
    "# You need to remove comments and empty lines (pre-processing)\n",
    "content=\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0338ec6-f016-4981-a7d5-8931917c1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "StartTime=time.time()\n",
    "Output = GenerateInferenceOutput(content)\n",
    "EndTime=time.time()\n",
    "InferenceTime=EndTime - StartTime\n",
    "print(Output)\n",
    "print()\n",
    "print(f\"Inference Time: {InferenceTime:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
